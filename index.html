<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1"> 
<title>AttnMod</title>

<meta property="og:title" content="AttnMod"/>
<meta property="og:type" content="article"/>
<meta property="og:description" content="Create unpromptable new art styles"/>
<meta property="og:image" content="https://attnmod.github.io/assets/img/android-chrome-192x192.png"/>

<link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
<link href="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.0.2/css/bootstrap.min.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
<link href="./assets/css/style.css" rel="stylesheet">
<link rel="icon" href="./assets/img/favicon.ico">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
</head>

<body>
<div class="container-fluid my-md-5">
<div class="content">
<div class="row py-3">
  <h1><strong>AttnMod</strong></h1>
  <h4 class="text-center my-1"><em>Attention-Based New Art Styles</em></h3>
  <p id="authors" class="py-3">
    <a href="https://www.linkedin.com/in/jessysu/">Shih-Chieh Su</a>
  </p>
  <div class="d-md-flex justify-content-evenly mt-5">
    <a href="" class="btn btn-dark text-light">
	  <span class="icon me-1"><i class="ai ai-arxiv"></i></span>
      <span>Paper</span>
    </a>
    <a href="" class="btn btn-dark text-light">
	  <span class="icon me-1"><i class="fab fa-github"></i></span>
      <span>Code</span>
    </a>
    <a href="#ref-block" class="btn btn-dark text-light">
	  <span class="icon me-1"><i class="fa-solid fa-arrow-right-from-bracket"></i></span>
      <span>Refs</span>
    </a>
  </div>
  <div class="container pt-5">
    <div class="col-12 text-center">
      <img class="img-fluid" src="./assets/img/SD15_ARHoM_s5_3x7_512.jpg"></img>
      <div class="text-start">
	    <span class="fw-bold">Fig. 1</span>
	    <span class="ms-1">Images from the same model, same checkpoints, same prompt, same seed, same guidance, same scheduler with same iterations. Why they look so different?</span>
	  </div>
    </div>
  </div>
</div>
</div>

<div class="content">
<div class="row">
  <h2 class="text-center">Abstract</h2>
  <p>
    The only difference among the images in Fig. 1 is the attention, which varies on one or few attention blocks of the diffuser UNet, then varies during the denoising loops. This work presents AttnMod, to create new unpromptable art styles out of existing diffusion models, and studies the style-creating behavior across different setups.
  </p>
</div>
</div>

<!--
<div class="content">
<div class="row">
  <h2>Background</h2>
  <p>
    The latent diffuser predicts better and better final outputs along denoising process, which can be used as animation frames.
    However, the majority of the update is completed during the first few frames -- making the rest of the animation stall (Vid. 1).
    In a different scenario, animation transits between two generated images via interpolating both the embedded prompts and seeded latents (Vid. 2).
    While the image-to-image animation provides high quality morphology, it also comes with occasional disruptions and nonsense.
  </p>
  <div class="d-md-flex justify-content-evenly">
  <div class="col-12 col-md-4 col-lg-3 text-center">
    <video class="img-fluid" src="./assets/mp4/astronaut diffuse 2fps 20s.mp4" autoplay controls muted loop playsinline></video>
    <div>Vid. 1</div>
  </div>
  <div class="col-12 col-md-4 col-lg-3 text-center">
    <video class="img-fluid" src="./assets/mp4/img2img 100slices slerp.mp4" autoplay controls muted loop playsinline></video>
    <div>Vid. 2</div>
  </div>
  </div>
</div>
</div>
-->

<div class="content">
<div class="row d-flex align-items-start">
  <h2>Approach</h2>
  <p class="col-12 col-md-6 col-xl-4">
    Imagine a human artist looking at the generated photo of a diffusion model, and hoping to create a painting out of it. There could be some feature of the object in the photo that the artist wants to emphasize, some color to disperse, some silhouette to twist, or some part of the scene to be materialized. These intentions can be viewed as the modification of the cross attention from the text prompt onto UNet, during the desoising diffusion. Motivated by the idea of creating new art styles mimicking the human artist in putting unusual emphasis on some composure component(s) of the scene, we design the attention modification process, AttnMod, as illustrated in Fig. 2
  </p>
  <div class="col-12 col-md-6 col-xl-8 text-center">
    <img class="img-fluid" src="./assets/img/attention_flow.jpg"></img>
    <div class="fw-bold">Fig. 2</div>
  </div>
  <p class="col-12 col-md-6 col-xl-12">
    In order to quantify the amount of attention used for conditioning, we introduce the attention multiplier. To better observe how the multiplier impacts the generation, we start with the simplest example. One arbitrary UNet attention block is selected to be modified, while the other blocks are fixed at the default attention which means a 1.0 multiplier. The selected block is then tested with the attention multiplier ranging from -20.0 to 50.0 at the beginning of the denoising diffusion. Within the denoising loop, the attention multiplier is further changed at a constant rate, ranging from -1.0 to 1.0. We select the "up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor", or U1A1A2 for short, to be the modified attention block out of the Stable Diffusion 1.5 (SD15). The AttnMod result is as shown in Fig. 3. The original generation, with a starting attention multiplier of 1.0 and change rate of 0.0 per loop, is marked out at the center. The starting multiplier increases toward the right, while the change rate increases toward the bottom.
  </p>
  <div class="col-12 text-center">
    <img class="img-fluid" src="./assets/img/SD15_ARHoM_s5_U1A1A2_7x11.jpg"></img>
    <div class="col-12 text-center"><span class="fw-bold mx-1">Fig. 3</span> U1A1A2 AttnMod scan</div>
  </div>
</div>
</div>

<div class="content">
<div class="row">
  <h2>Seeding</h2>
  <p>
    With fixed diffusion input, we check modding on numerous seeds using the constant-rate AttnMod mentioned above, to study how the seed impacts the generation. The output correlates with both the AttnMod setup and the default generation in the leftmost column in Fig. 4, which is again decided by the seed. Therefore, a good AttnMod strategy is to start from a prompt and seed combination that generates an image with good visual layout. 
  </p>
  <div class="col-12 text-center">
    <img class="img-fluid" src="./assets/img/seeds_6x1_1x12.jpg"></img>
    <div class="fw-bold">Fig. 4</div>
  </div>
</div>
</div>


<div class="content">
<div class="row">
  <h2>Styling</h2>
  <p>
    Experiments are conducted on applying AttnMod over various existing art styles. As shown in Fig. 5, the art styles are prompted in text and cover that of van Gogh, Ukiyo-E, Renoir, Picasso, da Vinci and Dali accordingly as rows. All the images share the same seed 0 and the same diffusion input except the style name, which is part of the text prompt. Images in the leftmost column are default diffusion output.
  </p>
  <div class="col-12 text-center">
    <img class="img-fluid" src="./assets/img/styles_6x1_1x12.jpg"></img>
    <div class="col-12 text-center"><span class="fw-bold mx-1">Fig. 5</span></div>
  </div>
  <p>
    An important observation is how the AttnMod setup correlates across different text prompts. Further away from the center when the text prompt loses its influence to a certain degree, the outcome does not resemble the style of Picasso nor van Gogh. It is a new style. There are few such new styles in Fig. 6.
  </p>
  <div class="d-md-flex justify-content-evenly">
  <div class="col-12 col-lg-6 text-center mx-1">
    <img class="img-fluid" src="./assets/img/SD15_ARHiS_Picasso_U1A0A2_7x11.jpg"></img>
	<div>(a) prompted Picasso</div>
  </div>
  <div class="col-12 col-lg-6 text-center mx-1">
    <img class="img-fluid" src="./assets/img/SD15_ARHiS_vanGogh_U1A0A2_7x11.jpg"></img>
	<div>(b) prompted van Gogh</div>
  </div>
  </div>
  <div class="col-12 text-center"><span class="fw-bold mx-1">Fig. 6</span> U1A0A2 AttnMod scan</div>
</div>
</div>


<div class="content">
<div class="row">
  <h2>Multi-block Modding</h2>
  <p>
    For SD15, we pick the two attention blocks within the same transformer block, and mod them synchronously with the same setup. Two of such combinations are visualized in Fig. 7 in rows, which contains the individual AttnMod scans from each of the two attention blocks, then the joined AttnMod scan of them toward the right.
  </p>
  <div class="col-12 text-center">
    <img class="img-fluid" src="./assets/img/L2only_M0A0_3600.jpg"></img>
    <img class="img-fluid" src="./assets/img/L2only_U1A2_3600.jpg"></img>
    <div class="fw-bold">Fig. 7</div>
  </div>
  <p>
    Beyond the two-block attention modding, there are many more combinations, as well as other asynchronous modding methods to be explored. In Fig. 8, we share a slice of possible exploration, where all 32 attention blocks in SD15 are opened up for modding and start with 0 attention. In each denoising loop, one block is picked to gain 1.0 attention. The largest attention that a block can have is capped at 1.0. For 40-loop denoising, we show results with two different approaches in picking blocks. The first one is to pick the available block causing the most difference in the output, compared to the previous output, with 1.0 attention gain. This approach attempts to include as much as the text "conditioning" within the strategy boundaries. On the other hand, the second approach avoids the influence as much as possible. Both approaches generate new art styles, so do further tweaking this strategy or devising more sophisticated strategies.
  </p>
  <div class="d-md-flex justify-content-evenly">
  <div class="col-12 col-md-4 col-lg-3 text-center">
    <img class="img-fluid" src="./assets/img/SD15_ARHoM_s0_allA_c1_1s_40d_1a_desc.png"></img>
  </div>
  <div class="col-12 col-md-4 col-lg-3 text-center">
    <img class="img-fluid" src="./assets/img/SD15_ARHoM_s0_allA_c1_1s_40d_1a_asc.png"></img>
  </div>
  </div>
  <div class="d-md-flex justify-content-evenly my-1">
  <div class="col-12 col-md-4 col-lg-3 text-center">
    <video class="img-fluid" src="./assets/mp4/SD15_ARHoM_s0_allA_c1_1s_40d_1a_desc.mp4" autoplay controls muted loop playsinline></video>
    <div class="text-center">(a) picking most influential</div>
  </div>
  <div class="col-12 col-md-4 col-lg-3 text-center">
    <video class="img-fluid" src="./assets/mp4/SD15_ARHoM_s0_allA_c1_1s_40d_1a_asc.mp4" autoplay controls muted loop playsinline></video>
    <div class="text-center">(b) picking least influential</div>
  </div>
  </div>
  <div class="col-12 text-start"><span class="fw-bold me-1">Fig. 8</span> Multi-block AttnMod using all 32 attention blocks in SD15. The only different parameter here is the approach in deciding which block gains attention during each denoising loop.</div>
</div>
</div>

<div class="content">
<div class="row">
  <h2>SDXL</h2>
  <p>
    Compared to the 32 attention blocks in SD15, SDXL has 140 blocks. In Fig. 9, we only show the AttnMod scan on two of them.
  </p>
  <div class="col-12 text-center">
    <img class="img-fluid" src="./assets/img/SDXL_2x1_7x12.jpg"></img>
    <div class="fw-bold">Fig. 9</div>
  </div>
</div>
</div>


<div class="content">
<div class="row">
  <h2 id="ref-block">References</h2>
  <ol>
    <li>
      Stable Diffusion 
	  [<a href="https://arxiv.org/abs/2112.10752">pdf</a>]
    </li>
    <li>
      CPIA: Channel Painter in Action 
	  [<a href="https://arxiv.org/pdf/1908.04694.pdf">pdf</a>]
      [<a href="https://github.com/jessysu/cpia">demo</a>]
    </li>
    <li>
      SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis
	  [<a href="https://arxiv.org/pdf/2307.01952">pdf</a>]
    </li>
  </ol>
</div>
</div>

<div class="content">
<div class="row">
  <h2>Bibtex</h2>
  <pre><code>@misc{su2024attnmod,
      title={AttnMod: Attention-Based New Art Styles}, 
      author={Shih-Chieh Su},
      year={2024},
      eprint={},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
  </code></pre>
</div>
</div>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.1.3/js/bootstrap.min.js"></script>

</body>
</html>
